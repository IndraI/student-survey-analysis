{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *n*-gram feature extraction and models, lemmatized texts, POS tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this nodebook are results from different *n*-gram models and setups while working with the lemmatized sentences and POS tags, only Catalan language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, nltk, numpy as np\n",
    "from nltk import bigrams, trigrams, ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_train_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imported file *get_train_test.py*. This file contains 3 functions:\n",
    "\n",
    "* **get_train_test(k=1,lemmatize=False,POS=False,language=\"es\")**\n",
    "\n",
    "Reads data from *txt* files. Returns two arrays of tuples (train and test), with sentences and their labels ('pos','neg' or 'neu').\n",
    "\n",
    "*lemmatize* - if *True* returns word lemmas\n",
    "\n",
    "*POS* - if *True* reurns words in form \"*lemma_POStag*\"\n",
    "\n",
    "* **get_train_test_comments(k=1,language=\"es\")**\n",
    "\n",
    "Reads data from *MongoDB* (as sentence order in comments is saved there). Returns two arrays of tuples (train and test), with comments and their labels ('pos','neg' or 'neu').\n",
    "\n",
    "* **get_english_train_test(k=1,language=\"es\")**\n",
    "\n",
    "Reads data from pre-created *txt* files with sentences translated to English. Returns two arrays of tuples (train and test), with comments and their labels ('pos','neg' or 'neu').\n",
    "\n",
    "\n",
    "For all the functions train-test split is 3/4 to 1/4, selection order depending on parameter *k*.\n",
    "\n",
    "*k* - takes values 1 to 4 - changes the selection of train-test split (used for cross-validation).\n",
    "\n",
    "*language* - 'es' or 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_list(text,remove_stopwords=False,repeat=False):\n",
    "    if remove_stopwords == True:\n",
    "        #with open(\"/data/es_stopwords.txt\") as f:\n",
    "        #    es_stopwords = f.readlines()\n",
    "        #es_stopwords = [x.strip() for x in es_stopwords] \n",
    "        with open(\"/data/ca_stopwords.txt\") as f:\n",
    "            ca_stopwords = f.readlines()\n",
    "        ca_stopwords = [x.strip() for x in ca_stopwords] \n",
    "        es_stopwords = set(stopwords.words(\"spanish\"))\n",
    "        for stopword in es_stopwords:\n",
    "            text = re.sub(stopword, '', text)\n",
    "        for stopword in ca_stopwords:\n",
    "            text = re.sub(stopword, '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.lower()\n",
    "    punctuation = ['.',',','%','&','\\'','/','+','!']\n",
    "    rx = '[' + re.escape(''.join(punctuation)) + ']'\n",
    "    text = re.sub(rx, '', text)\n",
    "    if repeat:\n",
    "        tokens = nltk.wordpunct_tokenize(text)\n",
    "    else:\n",
    "        tokens = sorted(set(nltk.wordpunct_tokenize(text)))\n",
    "    remove_from_vocabulary = ['-',')','(','(?)','1','=','[',']','][',':','<','>',';',':)','?']\n",
    "    for i in remove_from_vocabulary:\n",
    "        if i in tokens:\n",
    "            tokens.remove(i)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = get_word_list(text,remove_stopwords=False,repeat=True)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two functions are used to get vocabulary from bigram and trigram tokens. Can be changed to select bigrams or trigrams that appear more times than a given threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2grams_vocab(tokens):\n",
    "    bigr = list(bigrams(tokens))\n",
    "    fdist = nltk.FreqDist(bigr)\n",
    "    bi_grams = [x[0] for x in list(fdist.items()) if x[1] >= 1]\n",
    "    return bi_grams\n",
    "\n",
    "def get_3grams_vocab(tokens):\n",
    "    trigr = list(trigrams(tokens))\n",
    "    fdist = nltk.FreqDist(trigr)\n",
    "    tri_grams = [x[0] for x in list(fdist.items()) if x[1] >= 1]\n",
    "    return tri_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function creates feature vectors from ngrams.\n",
    "\n",
    "Ngramlist is a set of all ngrams (vocabulary).\n",
    "\n",
    "n - parameter for n-grams (2grams and 3grams use special functions above, larger n ngrams use nltk function ngrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_to_features(sentence, ngramlist, n=2):\n",
    "    tokens = tokenize(sentence)\n",
    "    if n==3:\n",
    "        sentence_ngrams = list(trigrams(tokens))\n",
    "    elif n == 4 or n==5:\n",
    "        sentence_ngrams = list(ngrams(tokens, n))\n",
    "    else:\n",
    "        sentence_ngrams = list(bigrams(tokens))\n",
    "    bag = np.zeros(len(ngramlist))\n",
    "    for w in sentence_ngrams:\n",
    "        for i,word in enumerate(ngramlist):\n",
    "            if word == w: \n",
    "                bag[i] += 1\n",
    "                \n",
    "    return np.array(bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function creates feature vectors from ngrams.\n",
    "\n",
    "n - parameter n for ngram\n",
    "\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LP_ngram_train_test_features(language=\"es\",POS=False,n=2,tf_idf=False,full_comments=False,k=1):\n",
    "    if POS==True:\n",
    "        train,test = get_train_test.get_train_test(k,POS=True,language=language)\n",
    "    else:\n",
    "        train,test = get_train_test.get_train_test(k,lemmatize=True,language=language)\n",
    "    train_corpus = \" \".join([i[0] for i in train])\n",
    "    tokens = tokenize(train_corpus)\n",
    "    if n==3:\n",
    "        ngramlist = get_3grams_vocab(tokens)\n",
    "    elif n > 3:\n",
    "        ngramlist = list(ngrams(tokens, n))\n",
    "    else:\n",
    "        ngramlist = get_2grams_vocab(tokens)\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for sentence in train:\n",
    "        f = ngrams_to_features(sentence[0],ngramlist,n)\n",
    "        X_train.append(f)\n",
    "        y_train.append(sentence[1])\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    for sentence in test:\n",
    "        f = ngrams_to_features(sentence[0],ngramlist,n)\n",
    "        X_test.append(f)\n",
    "        y_test.append(sentence[1])\n",
    "    if tf_idf == True:\n",
    "        transformer = TfidfTransformer()\n",
    "        X_train = transformer.fit_transform(X_train).toarray()\n",
    "        X_test = transformer.transform(X_test).toarray() \n",
    "    \n",
    "    return X_train,y_train,X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments for Catalan (lemmatized texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train models, sentence level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [1,2,3,4]\n",
    "random_states = [0,1,2,4,5,42,50,60,70,100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bigrams, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 2191 instances, test on 731 instances\n",
      "train on 2191 instances, test on 731 instances\n",
      "train on 2191 instances, test on 731 instances\n",
      "train on 2193 instances, test on 729 instances\n"
     ]
    }
   ],
   "source": [
    "results_svm = []\n",
    "results_lr = []\n",
    "for i in k:\n",
    "    X_train,y_train,X_test,y_test = get_LP_ngram_train_test_features(language=\"cat\",k=i,n=2)\n",
    "    accuracies = []\n",
    "    for s in random_states:\n",
    "        clf_svm = SGDClassifier(loss='hinge', alpha=1e-3, penalty='l2', random_state=s, max_iter=5,class_weight=\"balanced\")\n",
    "        clf_svm.fit(X_train, y_train) \n",
    "        predicted_svm = clf_svm.predict(X_test)\n",
    "        accuracies.append(np.mean(predicted_svm == y_test))\n",
    "    results_svm.append(np.mean(accuracies))\n",
    "    \n",
    "    clf_lr = LogisticRegression(max_iter=5,class_weight=\"balanced\").fit(X_train, y_train)\n",
    "    predicted_lr = clf_lr.predict(X_test)\n",
    "    results_lr.append(np.mean(predicted_lr == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi-grams, sentences\n",
      "------\n",
      "SVM accuracy:  0.6465922336877264\n",
      "SVM precision:  0.669636372452305\n",
      "SVM recall:  0.6750348675034867\n",
      "SVM F1:  0.6713324322250834\n",
      "SVM kappa:  0.4349977340521236\n",
      "------\n",
      "Logistic regression accuracy:  0.6668967586503735\n",
      "Logistic regression precision:  0.6615268204039437\n",
      "Logistic regression recall:  0.6569037656903766\n",
      "Logistic regression F1:  0.6590321522096558\n",
      "Logistic regression kappa:  0.4180962812425769\n"
     ]
    }
   ],
   "source": [
    "print(\"bi-grams, sentences\")\n",
    "print(\"------\")\n",
    "print(\"SVM accuracy: \", np.mean(results_svm))\n",
    "print(\"SVM precision: \", precision_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM recall: \", recall_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM F1: \", f1_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM kappa: \", cohen_kappa_score(y_test, predicted_svm))\n",
    "print(\"------\")\n",
    "print(\"Logistic regression accuracy: \", np.mean(results_lr))\n",
    "print(\"Logistic regression precision: \", precision_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression recall: \", recall_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression F1: \", f1_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression kappa: \", cohen_kappa_score(y_test, predicted_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.64976959, 0.2764977 , 0.07373272],\n",
       "       [0.10459184, 0.78061224, 0.11479592],\n",
       "       [0.15740741, 0.5       , 0.34259259]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = confusion_matrix(y_test, predicted_svm,['pos', 'neg','neu'])\n",
    "np.transpose( np.transpose(C) / C.astype(np.float).sum(axis=1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.64976959, 0.25345622, 0.09677419],\n",
       "       [0.125     , 0.73469388, 0.14030612],\n",
       "       [0.17592593, 0.43518519, 0.38888889]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = confusion_matrix(y_test, predicted_lr,['pos', 'neg','neu'])\n",
    "np.transpose( np.transpose(C) / C.astype(np.float).sum(axis=1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bigrams + TF-IDF, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 2191 instances, test on 731 instances\n",
      "train on 2191 instances, test on 731 instances\n",
      "train on 2191 instances, test on 731 instances\n",
      "train on 2193 instances, test on 729 instances\n"
     ]
    }
   ],
   "source": [
    "results_svm = []\n",
    "results_lr = []\n",
    "for i in k:\n",
    "    X_train,y_train,X_test,y_test = get_LP_ngram_train_test_features(language=\"cat\",k=i,n=2,tf_idf=True)\n",
    "    accuracies = []\n",
    "    for s in random_states:\n",
    "        clf_svm = SGDClassifier(loss='hinge', alpha=1e-3, penalty='l2', random_state=s, max_iter=5,class_weight=\"balanced\")\n",
    "        clf_svm.fit(X_train, y_train) \n",
    "        predicted_svm = clf_svm.predict(X_test)\n",
    "        accuracies.append(np.mean(predicted_svm == y_test))\n",
    "    results_svm.append(np.mean(accuracies))\n",
    "    \n",
    "    clf_lr = LogisticRegression(max_iter=5,class_weight=\"balanced\").fit(X_train, y_train)\n",
    "    predicted_lr = clf_lr.predict(X_test)\n",
    "    results_lr.append(np.mean(predicted_lr == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi-grams + tf-idf, sentences\n",
      "------\n",
      "SVM accuracy:  0.6536821612498218\n",
      "SVM precision:  0.6605367262395463\n",
      "SVM recall:  0.6847977684797768\n",
      "SVM F1:  0.6423214463435816\n",
      "SVM kappa:  0.39302383074998315\n",
      "------\n",
      "Logistic regression accuracy:  0.6794754921707944\n",
      "Logistic regression precision:  0.6585482909053342\n",
      "Logistic regression recall:  0.6861924686192469\n",
      "Logistic regression F1:  0.6353287702447842\n",
      "Logistic regression kappa:  0.39193314939617363\n"
     ]
    }
   ],
   "source": [
    "print(\"bi-grams + tf-idf, sentences\")\n",
    "print(\"------\")\n",
    "print(\"SVM accuracy: \", np.mean(results_svm))\n",
    "print(\"SVM precision: \", precision_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM recall: \", recall_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM F1: \", f1_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM kappa: \", cohen_kappa_score(y_test, predicted_svm))\n",
    "print(\"------\")\n",
    "print(\"Logistic regression accuracy: \", np.mean(results_lr))\n",
    "print(\"Logistic regression precision: \", precision_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression recall: \", recall_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression F1: \", f1_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression kappa: \", cohen_kappa_score(y_test, predicted_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.56221198, 0.41935484, 0.01843318],\n",
       "       [0.05357143, 0.91836735, 0.02806122],\n",
       "       [0.11111111, 0.80555556, 0.08333333]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = confusion_matrix(y_test, predicted_svm,['pos', 'neg','neu'])\n",
    "np.transpose( np.transpose(C) / C.astype(np.float).sum(axis=1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.57603687, 0.41474654, 0.00921659],\n",
       "       [0.06377551, 0.92346939, 0.0127551 ],\n",
       "       [0.13888889, 0.81481481, 0.0462963 ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = confusion_matrix(y_test, predicted_lr,['pos', 'neg','neu'])\n",
    "np.transpose( np.transpose(C) / C.astype(np.float).sum(axis=1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trigrams, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 2191 instances, test on 731 instances\n",
      "train on 2191 instances, test on 731 instances\n",
      "train on 2191 instances, test on 731 instances\n",
      "train on 2193 instances, test on 729 instances\n"
     ]
    }
   ],
   "source": [
    "results_svm = []\n",
    "results_lr = []\n",
    "for i in k:\n",
    "    X_train,y_train,X_test,y_test = get_LP_ngram_train_test_features(language=\"cat\",k=i,n=3)\n",
    "    accuracies = []\n",
    "    for s in random_states:\n",
    "        clf_svm = SGDClassifier(loss='hinge', alpha=1e-3, penalty='l2', random_state=s, max_iter=5,class_weight=\"balanced\")\n",
    "        clf_svm.fit(X_train, y_train) \n",
    "        predicted_svm = clf_svm.predict(X_test)\n",
    "        accuracies.append(np.mean(predicted_svm == y_test))\n",
    "    results_svm.append(np.mean(accuracies))\n",
    "    \n",
    "    clf_lr = LogisticRegression(max_iter=5,class_weight=\"balanced\").fit(X_train, y_train)\n",
    "    predicted_lr = clf_lr.predict(X_test)\n",
    "    results_lr.append(np.mean(predicted_lr == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tri-grams, sentences\n",
      "------\n",
      "SVM accuracy:  0.5732557565074381\n",
      "SVM precision:  0.5375803319933273\n",
      "SVM recall:  0.5718270571827058\n",
      "SVM F1:  0.5466119944063426\n",
      "SVM kappa:  0.21402633016614359\n",
      "------\n",
      "Logistic regression accuracy:  0.6035565717759458\n",
      "Logistic regression precision:  0.5571885215717153\n",
      "Logistic regression recall:  0.595536959553696\n",
      "Logistic regression F1:  0.561950918768401\n",
      "Logistic regression kappa:  0.24102058694699957\n"
     ]
    }
   ],
   "source": [
    "print(\"tri-grams, sentences\")\n",
    "print(\"------\")\n",
    "print(\"SVM accuracy: \", np.mean(results_svm))\n",
    "print(\"SVM precision: \", precision_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM recall: \", recall_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM F1: \", f1_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM kappa: \", cohen_kappa_score(y_test, predicted_svm))\n",
    "print(\"------\")\n",
    "print(\"Logistic regression accuracy: \", np.mean(results_lr))\n",
    "print(\"Logistic regression precision: \", precision_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression recall: \", recall_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression F1: \", f1_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression kappa: \", cohen_kappa_score(y_test, predicted_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46082949, 0.49308756, 0.04608295],\n",
       "       [0.16581633, 0.76020408, 0.07397959],\n",
       "       [0.19444444, 0.69444444, 0.11111111]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = confusion_matrix(y_test, predicted_svm,['pos', 'neg','neu'])\n",
    "np.transpose( np.transpose(C) / C.astype(np.float).sum(axis=1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4562212 , 0.51152074, 0.03225806],\n",
       "       [0.13010204, 0.81122449, 0.05867347],\n",
       "       [0.17592593, 0.73148148, 0.09259259]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = confusion_matrix(y_test, predicted_lr,['pos', 'neg','neu'])\n",
    "np.transpose( np.transpose(C) / C.astype(np.float).sum(axis=1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trigrams + TF-IDF, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 2191 instances, test on 731 instances\n",
      "train on 2191 instances, test on 731 instances\n",
      "train on 2191 instances, test on 731 instances\n",
      "train on 2193 instances, test on 729 instances\n"
     ]
    }
   ],
   "source": [
    "results_svm = []\n",
    "results_lr = []\n",
    "for i in k:\n",
    "    X_train,y_train,X_test,y_test = get_LP_ngram_train_test_features(language=\"cat\",k=i,n=3,tf_idf=True)\n",
    "    accuracies = []\n",
    "    for s in random_states:\n",
    "        clf_svm = SGDClassifier(loss='hinge', alpha=1e-3, penalty='l2', random_state=s, max_iter=5,class_weight=\"balanced\")\n",
    "        clf_svm.fit(X_train, y_train) \n",
    "        predicted_svm = clf_svm.predict(X_test)\n",
    "        accuracies.append(np.mean(predicted_svm == y_test))\n",
    "    results_svm.append(np.mean(accuracies))\n",
    "    \n",
    "    clf_lr = LogisticRegression(max_iter=5,class_weight=\"balanced\").fit(X_train, y_train)\n",
    "    predicted_lr = clf_lr.predict(X_test)\n",
    "    results_lr.append(np.mean(predicted_lr == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tri-grams + tf-idf, sentences\n",
      "------\n",
      "SVM accuracy:  0.557560270441209\n",
      "SVM precision:  0.5982428099496642\n",
      "SVM recall:  0.6178521617852162\n",
      "SVM F1:  0.5629812678938507\n",
      "SVM kappa:  0.2422531299900489\n",
      "------\n",
      "Logistic regression accuracy:  0.6084567013940458\n",
      "Logistic regression precision:  0.5794773304067494\n",
      "Logistic regression recall:  0.6136680613668062\n",
      "Logistic regression F1:  0.5412452996949276\n",
      "Logistic regression kappa:  0.2141486402538667\n"
     ]
    }
   ],
   "source": [
    "print(\"tri-grams + tf-idf, sentences\")\n",
    "print(\"------\")\n",
    "print(\"SVM accuracy: \", np.mean(results_svm))\n",
    "print(\"SVM precision: \", precision_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM recall: \", recall_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM F1: \", f1_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM kappa: \", cohen_kappa_score(y_test, predicted_svm))\n",
    "print(\"------\")\n",
    "print(\"Logistic regression accuracy: \", np.mean(results_lr))\n",
    "print(\"Logistic regression precision: \", precision_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression recall: \", recall_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression F1: \", f1_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression kappa: \", cohen_kappa_score(y_test, predicted_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.359447  , 0.61290323, 0.02764977],\n",
       "       [0.06377551, 0.91071429, 0.0255102 ],\n",
       "       [0.07407407, 0.85185185, 0.07407407]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = confusion_matrix(y_test, predicted_svm,['pos', 'neg','neu'])\n",
    "np.transpose( np.transpose(C) / C.astype(np.float).sum(axis=1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32258065, 0.65898618, 0.01843318],\n",
       "       [0.05357143, 0.93877551, 0.00765306],\n",
       "       [0.06481481, 0.91666667, 0.01851852]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = confusion_matrix(y_test, predicted_lr,['pos', 'neg','neu'])\n",
    "np.transpose( np.transpose(C) / C.astype(np.float).sum(axis=1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-grams, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 2191 instances, test on 731 instances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/indra/anaconda/envs/icutestenv/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 2191 instances, test on 731 instances\n",
      "train on 2191 instances, test on 731 instances\n",
      "train on 2193 instances, test on 729 instances\n"
     ]
    }
   ],
   "source": [
    "results_svm = []\n",
    "results_lr = []\n",
    "for i in k:\n",
    "    X_train,y_train,X_test,y_test = get_LP_ngram_train_test_features(language=\"cat\",k=i,n=4)\n",
    "    accuracies = []\n",
    "    for s in random_states:\n",
    "        clf_svm = SGDClassifier(loss='hinge', alpha=1e-3, penalty='l2', random_state=s, max_iter=5,class_weight=\"balanced\")\n",
    "        clf_svm.fit(X_train, y_train) \n",
    "        predicted_svm = clf_svm.predict(X_test)\n",
    "        accuracies.append(np.mean(predicted_svm == y_test))\n",
    "    results_svm.append(np.mean(accuracies))\n",
    "    \n",
    "    clf_lr = LogisticRegression(max_iter=5,class_weight=\"balanced\").fit(X_train, y_train)\n",
    "    predicted_lr = clf_lr.predict(X_test)\n",
    "    results_lr.append(np.mean(predicted_lr == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-grams, sentences\n",
      "------\n",
      "SVM accuracy:  0.4953629607215222\n",
      "SVM precision:  0.5255951387295758\n",
      "SVM recall:  0.5718270571827058\n",
      "SVM F1:  0.5048760204128034\n",
      "SVM kappa:  0.14026090692496984\n",
      "------\n",
      "Logistic regression accuracy:  0.567164305602418\n",
      "Logistic regression precision:  0.5437925461180383\n",
      "Logistic regression recall:  0.5774058577405857\n",
      "Logistic regression F1:  0.5168692736537541\n",
      "Logistic regression kappa:  0.15786025777691637\n"
     ]
    }
   ],
   "source": [
    "print(\"4-grams, sentences\")\n",
    "print(\"------\")\n",
    "print(\"SVM accuracy: \", np.mean(results_svm))\n",
    "print(\"SVM precision: \", precision_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM recall: \", recall_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM F1: \", f1_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM kappa: \", cohen_kappa_score(y_test, predicted_svm))\n",
    "print(\"------\")\n",
    "print(\"Logistic regression accuracy: \", np.mean(results_lr))\n",
    "print(\"Logistic regression precision: \", precision_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression recall: \", recall_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression F1: \", f1_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression kappa: \", cohen_kappa_score(y_test, predicted_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26728111, 0.71428571, 0.01843318],\n",
       "       [0.08163265, 0.8877551 , 0.03061224],\n",
       "       [0.08333333, 0.87962963, 0.03703704]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = confusion_matrix(y_test, predicted_svm,['pos', 'neg','neu'])\n",
    "np.transpose( np.transpose(C) / C.astype(np.float).sum(axis=1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.28110599, 0.70046083, 0.01843318],\n",
       "       [0.08163265, 0.88265306, 0.03571429],\n",
       "       [0.08333333, 0.85185185, 0.06481481]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = confusion_matrix(y_test, predicted_lr,['pos', 'neg','neu'])\n",
    "np.transpose( np.transpose(C) / C.astype(np.float).sum(axis=1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-grams + TF-IDF, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 2191 instances, test on 731 instances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/indra/anaconda/envs/icutestenv/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 2191 instances, test on 731 instances\n",
      "train on 2191 instances, test on 731 instances\n",
      "train on 2193 instances, test on 729 instances\n"
     ]
    }
   ],
   "source": [
    "results_svm = []\n",
    "results_lr = []\n",
    "for i in k:\n",
    "    X_train,y_train,X_test,y_test = get_LP_ngram_train_test_features(language=\"cat\",k=i,n=4,tf_idf=True)\n",
    "    accuracies = []\n",
    "    for s in random_states:\n",
    "        clf_svm = SGDClassifier(loss='hinge', alpha=1e-3, penalty='l2', random_state=s, max_iter=5,class_weight=\"balanced\")\n",
    "        clf_svm.fit(X_train, y_train) \n",
    "        predicted_svm = clf_svm.predict(X_test)\n",
    "        accuracies.append(np.mean(predicted_svm == y_test))\n",
    "    results_svm.append(np.mean(accuracies))\n",
    "    \n",
    "    clf_lr = LogisticRegression(max_iter=5,class_weight=\"balanced\").fit(X_train, y_train)\n",
    "    predicted_lr = clf_lr.predict(X_test)\n",
    "    results_lr.append(np.mean(predicted_lr == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-grams + tf-idf, sentences\n",
      "------\n",
      "SVM accuracy:  0.4962508718676295\n",
      "SVM precision:  0.5590759837028739\n",
      "SVM recall:  0.5760111576011158\n",
      "SVM F1:  0.49183089249491985\n",
      "SVM kappa:  0.12657330047484516\n",
      "------\n",
      "Logistic regression accuracy:  0.5738342097380285\n",
      "Logistic regression precision:  0.5598588428870056\n",
      "Logistic regression recall:  0.5760111576011158\n",
      "Logistic regression F1:  0.4779091600459741\n",
      "Logistic regression kappa:  0.11044724953169194\n"
     ]
    }
   ],
   "source": [
    "print(\"4-grams + tf-idf, sentences\")\n",
    "print(\"------\")\n",
    "print(\"SVM accuracy: \", np.mean(results_svm))\n",
    "print(\"SVM precision: \", precision_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM recall: \", recall_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM F1: \", f1_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM kappa: \", cohen_kappa_score(y_test, predicted_svm))\n",
    "print(\"------\")\n",
    "print(\"Logistic regression accuracy: \", np.mean(results_lr))\n",
    "print(\"Logistic regression precision: \", precision_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression recall: \", recall_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression F1: \", f1_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression kappa: \", cohen_kappa_score(y_test, predicted_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1797235 , 0.80184332, 0.01843318],\n",
       "       [0.04081633, 0.93622449, 0.02295918],\n",
       "       [0.06481481, 0.87037037, 0.06481481]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = confusion_matrix(y_test, predicted_svm,['pos', 'neg','neu'])\n",
    "np.transpose( np.transpose(C) / C.astype(np.float).sum(axis=1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16129032, 0.82488479, 0.01382488],\n",
       "       [0.03571429, 0.95663265, 0.00765306],\n",
       "       [0.0462963 , 0.92592593, 0.02777778]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = confusion_matrix(y_test, predicted_lr,['pos', 'neg','neu'])\n",
    "np.transpose( np.transpose(C) / C.astype(np.float).sum(axis=1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-grams, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 2191 instances, test on 731 instances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/indra/anaconda/envs/icutestenv/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 2191 instances, test on 731 instances\n",
      "train on 2191 instances, test on 731 instances\n",
      "train on 2193 instances, test on 729 instances\n"
     ]
    }
   ],
   "source": [
    "results_svm = []\n",
    "results_lr = []\n",
    "for i in k:\n",
    "    X_train,y_train,X_test,y_test = get_LP_ngram_train_test_features(language=\"cat\",k=i,n=5)\n",
    "    accuracies = []\n",
    "    for s in random_states:\n",
    "        clf_svm = SGDClassifier(loss='hinge', alpha=1e-3, penalty='l2', random_state=s, max_iter=5,class_weight=\"balanced\")\n",
    "        clf_svm.fit(X_train, y_train) \n",
    "        predicted_svm = clf_svm.predict(X_test)\n",
    "        accuracies.append(np.mean(predicted_svm == y_test))\n",
    "    results_svm.append(np.mean(accuracies))\n",
    "    \n",
    "    clf_lr = LogisticRegression(max_iter=5,class_weight=\"balanced\").fit(X_train, y_train)\n",
    "    predicted_lr = clf_lr.predict(X_test)\n",
    "    results_lr.append(np.mean(predicted_lr == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-grams, sentences\n",
      "------\n",
      "SVM accuracy:  0.47049681629682694\n",
      "SVM precision:  0.5524071121515042\n",
      "SVM recall:  0.5648535564853556\n",
      "SVM F1:  0.44544870417716875\n",
      "SVM kappa:  0.0669708584799028\n",
      "------\n",
      "Logistic regression accuracy:  0.5671731822015176\n",
      "Logistic regression precision:  0.5996004557361386\n",
      "Logistic regression recall:  0.5815899581589958\n",
      "Logistic regression F1:  0.47887088411463896\n",
      "Logistic regression kappa:  0.11433371213992805\n"
     ]
    }
   ],
   "source": [
    "print(\"5-grams, sentences\")\n",
    "print(\"------\")\n",
    "print(\"SVM accuracy: \", np.mean(results_svm))\n",
    "print(\"SVM precision: \", precision_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM recall: \", recall_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM F1: \", f1_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM kappa: \", cohen_kappa_score(y_test, predicted_svm))\n",
    "print(\"------\")\n",
    "print(\"Logistic regression accuracy: \", np.mean(results_lr))\n",
    "print(\"Logistic regression precision: \", precision_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression recall: \", recall_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression F1: \", f1_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression kappa: \", cohen_kappa_score(y_test, predicted_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10138249, 0.88940092, 0.00921659],\n",
       "       [0.02295918, 0.9744898 , 0.00255102],\n",
       "       [0.00925926, 0.98148148, 0.00925926]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = confusion_matrix(y_test, predicted_svm,['pos', 'neg','neu'])\n",
    "np.transpose( np.transpose(C) / C.astype(np.float).sum(axis=1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17050691, 0.82488479, 0.00460829],\n",
       "       [0.03316327, 0.96683673, 0.        ],\n",
       "       [0.02777778, 0.96296296, 0.00925926]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = confusion_matrix(y_test, predicted_lr,['pos', 'neg','neu'])\n",
    "np.transpose( np.transpose(C) / C.astype(np.float).sum(axis=1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-grams + TF-IDF, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 2191 instances, test on 731 instances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/indra/anaconda/envs/icutestenv/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 2191 instances, test on 731 instances\n",
      "train on 2191 instances, test on 731 instances\n",
      "train on 2193 instances, test on 729 instances\n"
     ]
    }
   ],
   "source": [
    "results_svm = []\n",
    "results_lr = []\n",
    "for i in k:\n",
    "    X_train,y_train,X_test,y_test = get_LP_ngram_train_test_features(language=\"cat\",k=i,n=5,tf_idf=True)\n",
    "    accuracies = []\n",
    "    for s in random_states:\n",
    "        clf_svm = SGDClassifier(loss='hinge', alpha=1e-3, penalty='l2', random_state=s, max_iter=5,class_weight=\"balanced\")\n",
    "        clf_svm.fit(X_train, y_train) \n",
    "        predicted_svm = clf_svm.predict(X_test)\n",
    "        accuracies.append(np.mean(predicted_svm == y_test))\n",
    "    results_svm.append(np.mean(accuracies))\n",
    "    \n",
    "    clf_lr = LogisticRegression(max_iter=5,class_weight=\"balanced\").fit(X_train, y_train)\n",
    "    predicted_lr = clf_lr.predict(X_test)\n",
    "    results_lr.append(np.mean(predicted_lr == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-grams + tf-idf, sentences\n",
      "------\n",
      "SVM accuracy:  0.476421278290353\n",
      "SVM precision:  0.55336052970475\n",
      "SVM recall:  0.5690376569037657\n",
      "SVM F1:  0.4595008443130009\n",
      "SVM kappa:  0.0857761822233225\n",
      "------\n",
      "Logistic regression accuracy:  0.5570263137870786\n",
      "Logistic regression precision:  0.5916027011027162\n",
      "Logistic regression recall:  0.5606694560669456\n",
      "Logistic regression F1:  0.42778934755581977\n",
      "Logistic regression kappa:  0.047399121863572424\n"
     ]
    }
   ],
   "source": [
    "print(\"5-grams + tf-idf, sentences\")\n",
    "print(\"------\")\n",
    "print(\"SVM accuracy: \", np.mean(results_svm))\n",
    "print(\"SVM precision: \", precision_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM recall: \", recall_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM F1: \", f1_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM kappa: \", cohen_kappa_score(y_test, predicted_svm))\n",
    "print(\"------\")\n",
    "print(\"Logistic regression accuracy: \", np.mean(results_lr))\n",
    "print(\"Logistic regression precision: \", precision_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression recall: \", recall_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression F1: \", f1_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression kappa: \", cohen_kappa_score(y_test, predicted_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.12903226, 0.85253456, 0.01843318],\n",
       "       [0.02040816, 0.96683673, 0.0127551 ],\n",
       "       [0.00925926, 0.98148148, 0.00925926]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = confusion_matrix(y_test, predicted_svm,['pos', 'neg','neu'])\n",
    "np.transpose( np.transpose(C) / C.astype(np.float).sum(axis=1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06451613, 0.93087558, 0.00460829],\n",
       "       [0.0127551 , 0.9872449 , 0.        ],\n",
       "       [0.00925926, 0.98148148, 0.00925926]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = confusion_matrix(y_test, predicted_lr,['pos', 'neg','neu'])\n",
    "np.transpose( np.transpose(C) / C.astype(np.float).sum(axis=1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bigrams, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 2191 instances, test on 731 instances\n",
      "train on 2191 instances, test on 731 instances\n",
      "train on 2191 instances, test on 731 instances\n",
      "train on 2193 instances, test on 729 instances\n"
     ]
    }
   ],
   "source": [
    "results_svm = []\n",
    "results_lr = []\n",
    "for i in k:\n",
    "    X_train,y_train,X_test,y_test = get_LP_ngram_train_test_features(language=\"cat\",k=i,n=2,POS=True)\n",
    "    accuracies = []\n",
    "    for s in random_states:\n",
    "        clf_svm = SGDClassifier(loss='hinge', alpha=1e-3, penalty='l2', random_state=s, max_iter=5,class_weight=\"balanced\")\n",
    "        clf_svm.fit(X_train, y_train) \n",
    "        predicted_svm = clf_svm.predict(X_test)\n",
    "        accuracies.append(np.mean(predicted_svm == y_test))\n",
    "    results_svm.append(np.mean(accuracies))\n",
    "\n",
    "    clf_lr = LogisticRegression(max_iter=5,class_weight=\"balanced\").fit(X_train, y_train)\n",
    "    predicted_lr = clf_lr.predict(X_test)\n",
    "    results_lr.append(np.mean(predicted_lr == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tags bigram features, sentences\n",
      "------\n",
      "SVM accuracy:  0.4918112397991642\n",
      "SVM precision:  0.5567885822580932\n",
      "SVM recall:  0.5871687587168759\n",
      "SVM F1:  0.5413706363927735\n",
      "SVM kappa:  0.21569844789356984\n",
      "------\n",
      "Logistic regression accuracy:  0.5646911645163857\n",
      "Logistic regression precision:  0.5868039811361114\n",
      "Logistic regression recall:  0.5746164574616457\n",
      "Logistic regression F1:  0.5798612740992576\n",
      "Logistic regression kappa:  0.2897462463096425\n"
     ]
    }
   ],
   "source": [
    "print(\"POS tags bigram features, sentences\")\n",
    "print(\"------\")\n",
    "print(\"SVM accuracy: \", np.mean(results_svm))\n",
    "print(\"SVM precision: \", precision_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM recall: \", recall_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM F1: \", f1_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM kappa: \", cohen_kappa_score(y_test, predicted_svm))\n",
    "print(\"------\")\n",
    "print(\"Logistic regression accuracy: \", np.mean(results_lr))\n",
    "print(\"Logistic regression precision: \", precision_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression recall: \", recall_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression F1: \", f1_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression kappa: \", cohen_kappa_score(y_test, predicted_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.21658986, 0.640553  , 0.14285714],\n",
       "       [0.09183673, 0.875     , 0.03316327],\n",
       "       [0.09259259, 0.62037037, 0.28703704]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = confusion_matrix(y_test, predicted_svm,['pos', 'neg','neu'])\n",
    "np.transpose( np.transpose(C) / C.astype(np.float).sum(axis=1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49769585, 0.31797235, 0.1843318 ],\n",
       "       [0.20153061, 0.66836735, 0.13010204],\n",
       "       [0.23148148, 0.37962963, 0.38888889]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = confusion_matrix(y_test, predicted_lr,['pos', 'neg','neu'])\n",
    "np.transpose( np.transpose(C) / C.astype(np.float).sum(axis=1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bigrams + TF-IDF, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 2191 instances, test on 731 instances\n",
      "train on 2191 instances, test on 731 instances\n",
      "train on 2191 instances, test on 731 instances\n",
      "train on 2193 instances, test on 729 instances\n"
     ]
    }
   ],
   "source": [
    "results_svm = []\n",
    "results_lr = []\n",
    "for i in k:\n",
    "    X_train,y_train,X_test,y_test = get_LP_ngram_train_test_features(language=\"cat\",k=i,n=2,POS=True,tf_idf=True)\n",
    "    accuracies = []\n",
    "    for s in random_states:\n",
    "        clf_svm = SGDClassifier(loss='hinge', alpha=1e-3, penalty='l2', random_state=s, max_iter=5,class_weight=\"balanced\")\n",
    "        clf_svm.fit(X_train, y_train) \n",
    "        predicted_svm = clf_svm.predict(X_test)\n",
    "        accuracies.append(np.mean(predicted_svm == y_test))\n",
    "    results_svm.append(np.mean(accuracies))\n",
    "\n",
    "    clf_lr = LogisticRegression(max_iter=5,class_weight=\"balanced\").fit(X_train, y_train)\n",
    "    predicted_lr = clf_lr.predict(X_test)\n",
    "    results_lr.append(np.mean(predicted_lr == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tags bigram + tf-idf features, sentences\n",
      "------\n",
      "SVM accuracy:  0.5768032555925146\n",
      "SVM precision:  0.5940463808646673\n",
      "SVM recall:  0.599721059972106\n",
      "SVM F1:  0.5787066168571915\n",
      "SVM kappa:  0.2795735816689132\n",
      "------\n",
      "Logistic regression accuracy:  0.5881709316935205\n",
      "Logistic regression precision:  0.5842434709920424\n",
      "Logistic regression recall:  0.5885634588563459\n",
      "Logistic regression F1:  0.5854775859036804\n",
      "Logistic regression kappa:  0.29015618812379596\n"
     ]
    }
   ],
   "source": [
    "print(\"POS tags bigram + tf-idf features, sentences\")\n",
    "print(\"------\")\n",
    "print(\"SVM accuracy: \", np.mean(results_svm))\n",
    "print(\"SVM precision: \", precision_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM recall: \", recall_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM F1: \", f1_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM kappa: \", cohen_kappa_score(y_test, predicted_svm))\n",
    "print(\"------\")\n",
    "print(\"Logistic regression accuracy: \", np.mean(results_lr))\n",
    "print(\"Logistic regression precision: \", precision_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression recall: \", recall_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression F1: \", f1_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression kappa: \", cohen_kappa_score(y_test, predicted_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = confusion_matrix(y_test, predicted_svm,['pos', 'neg','neu'])\n",
    "np.transpose( np.transpose(C) / C.astype(np.float).sum(axis=1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = confusion_matrix(y_test, predicted_lr,['pos', 'neg','neu'])\n",
    "np.transpose( np.transpose(C) / C.astype(np.float).sum(axis=1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trigrams, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 2191 instances, test on 731 instances\n",
      "train on 2191 instances, test on 731 instances\n",
      "train on 2191 instances, test on 731 instances\n",
      "train on 2193 instances, test on 729 instances\n"
     ]
    }
   ],
   "source": [
    "results_svm = []\n",
    "results_lr = []\n",
    "for i in k:\n",
    "    X_train,y_train,X_test,y_test = get_LP_ngram_train_test_features(language=\"cat\",k=i,n=3,POS=True)\n",
    "    accuracies = []\n",
    "    for s in random_states:\n",
    "        clf_svm = SGDClassifier(loss='hinge', alpha=1e-3, penalty='l2', random_state=s, max_iter=5,class_weight=\"balanced\")\n",
    "        clf_svm.fit(X_train, y_train) \n",
    "        predicted_svm = clf_svm.predict(X_test)\n",
    "        accuracies.append(np.mean(predicted_svm == y_test))\n",
    "    results_svm.append(np.mean(accuracies))\n",
    "\n",
    "    clf_lr = LogisticRegression(max_iter=5,class_weight=\"balanced\").fit(X_train, y_train)\n",
    "    predicted_lr = clf_lr.predict(X_test)\n",
    "    results_lr.append(np.mean(predicted_lr == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tags trigram features, sentences\n",
      "------\n",
      "SVM accuracy:  0.5200328537047991\n",
      "SVM precision:  0.572754116843014\n",
      "SVM recall:  0.5202231520223152\n",
      "SVM F1:  0.5216348081438825\n",
      "SVM kappa:  0.2085178482036274\n",
      "------\n",
      "Logistic regression accuracy:  0.5468353586753036\n",
      "Logistic regression precision:  0.5878432977619156\n",
      "Logistic regression recall:  0.5690376569037657\n",
      "Logistic regression F1:  0.5766860728120499\n",
      "Logistic regression kappa:  0.28519300011614845\n"
     ]
    }
   ],
   "source": [
    "print(\"POS tags trigram features, sentences\")\n",
    "print(\"------\")\n",
    "print(\"SVM accuracy: \", np.mean(results_svm))\n",
    "print(\"SVM precision: \", precision_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM recall: \", recall_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM F1: \", f1_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM kappa: \", cohen_kappa_score(y_test, predicted_svm))\n",
    "print(\"------\")\n",
    "print(\"Logistic regression accuracy: \", np.mean(results_lr))\n",
    "print(\"Logistic regression precision: \", precision_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression recall: \", recall_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression F1: \", f1_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression kappa: \", cohen_kappa_score(y_test, predicted_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26267281, 0.41935484, 0.31797235],\n",
       "       [0.07653061, 0.67602041, 0.24744898],\n",
       "       [0.09259259, 0.43518519, 0.47222222]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = confusion_matrix(y_test, predicted_svm,['pos', 'neg','neu'])\n",
    "np.transpose( np.transpose(C) / C.astype(np.float).sum(axis=1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.48387097, 0.31336406, 0.20276498],\n",
       "       [0.17857143, 0.66581633, 0.15561224],\n",
       "       [0.24074074, 0.37037037, 0.38888889]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = confusion_matrix(y_test, predicted_lr,['pos', 'neg','neu'])\n",
    "np.transpose( np.transpose(C) / C.astype(np.float).sum(axis=1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trigrams + TF-IDF, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 2191 instances, test on 731 instances\n",
      "train on 2191 instances, test on 731 instances\n",
      "train on 2191 instances, test on 731 instances\n",
      "train on 2193 instances, test on 729 instances\n"
     ]
    }
   ],
   "source": [
    "results_svm = []\n",
    "results_lr = []\n",
    "for i in k:\n",
    "    X_train,y_train,X_test,y_test = get_LP_ngram_train_test_features(language=\"cat\",k=i,n=3,POS=True,tf_idf=True)\n",
    "    accuracies = []\n",
    "    for s in random_states:\n",
    "        clf_svm = SGDClassifier(loss='hinge', alpha=1e-3, penalty='l2', random_state=s, max_iter=5,class_weight=\"balanced\")\n",
    "        clf_svm.fit(X_train, y_train) \n",
    "        predicted_svm = clf_svm.predict(X_test)\n",
    "        accuracies.append(np.mean(predicted_svm == y_test))\n",
    "    results_svm.append(np.mean(accuracies))\n",
    "\n",
    "    clf_lr = LogisticRegression(max_iter=5,class_weight=\"balanced\").fit(X_train, y_train)\n",
    "    predicted_lr = clf_lr.predict(X_test)\n",
    "    results_lr.append(np.mean(predicted_lr == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tags trigram + tf-idf features, sentences\n",
      "------\n",
      "SVM accuracy:  0.5750419087000835\n",
      "SVM precision:  0.5969169279864831\n",
      "SVM recall:  0.599721059972106\n",
      "SVM F1:  0.5737304372667352\n",
      "SVM kappa:  0.2680915085682579\n",
      "------\n",
      "Logistic regression accuracy:  0.5867202929302394\n",
      "Logistic regression precision:  0.5935779700244853\n",
      "Logistic regression recall:  0.603905160390516\n",
      "Logistic regression F1:  0.5930221984059234\n",
      "Logistic regression kappa:  0.2971291882544208\n"
     ]
    }
   ],
   "source": [
    "print(\"POS tags trigram + tf-idf features, sentences\")\n",
    "print(\"------\")\n",
    "print(\"SVM accuracy: \", np.mean(results_svm))\n",
    "print(\"SVM precision: \", precision_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM recall: \", recall_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM F1: \", f1_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM kappa: \", cohen_kappa_score(y_test, predicted_svm))\n",
    "print(\"------\")\n",
    "print(\"Logistic regression accuracy: \", np.mean(results_lr))\n",
    "print(\"Logistic regression precision: \", precision_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression recall: \", recall_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression F1: \", f1_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression kappa: \", cohen_kappa_score(y_test, predicted_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29953917, 0.55760369, 0.14285714],\n",
       "       [0.07142857, 0.83418367, 0.09438776],\n",
       "       [0.09259259, 0.55555556, 0.35185185]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = confusion_matrix(y_test, predicted_svm,['pos', 'neg','neu'])\n",
    "np.transpose( np.transpose(C) / C.astype(np.float).sum(axis=1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.41935484, 0.47004608, 0.11059908],\n",
       "       [0.13010204, 0.77295918, 0.09693878],\n",
       "       [0.16666667, 0.47222222, 0.36111111]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = confusion_matrix(y_test, predicted_lr,['pos', 'neg','neu'])\n",
    "np.transpose( np.transpose(C) / C.astype(np.float).sum(axis=1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-grams, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 2191 instances, test on 731 instances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/indra/anaconda/envs/icutestenv/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 2191 instances, test on 731 instances\n",
      "train on 2191 instances, test on 731 instances\n",
      "train on 2193 instances, test on 729 instances\n"
     ]
    }
   ],
   "source": [
    "results_svm = []\n",
    "results_lr = []\n",
    "for i in k:\n",
    "    X_train,y_train,X_test,y_test = get_LP_ngram_train_test_features(language=\"cat\",k=i,n=4,POS=True)\n",
    "    accuracies = []\n",
    "    for s in random_states:\n",
    "        clf_svm = SGDClassifier(loss='hinge', alpha=1e-3, penalty='l2', random_state=s, max_iter=5,class_weight=\"balanced\")\n",
    "        clf_svm.fit(X_train, y_train) \n",
    "        predicted_svm = clf_svm.predict(X_test)\n",
    "        accuracies.append(np.mean(predicted_svm == y_test))\n",
    "    results_svm.append(np.mean(accuracies))\n",
    "\n",
    "    clf_lr = LogisticRegression(max_iter=5,class_weight=\"balanced\").fit(X_train, y_train)\n",
    "    predicted_lr = clf_lr.predict(X_test)\n",
    "    results_lr.append(np.mean(predicted_lr == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tags 4-gram features, sentences\n",
      "------\n",
      "SVM accuracy:  0.47472673881444605\n",
      "SVM precision:  0.5260280955568072\n",
      "SVM recall:  0.5285913528591353\n",
      "SVM F1:  0.5221137187238404\n",
      "SVM kappa:  0.18725748952824273\n",
      "------\n",
      "Logistic regression accuracy:  0.5339051064807115\n",
      "Logistic regression precision:  0.5697842166927188\n",
      "Logistic regression recall:  0.5564853556485355\n",
      "Logistic regression F1:  0.5623266576158675\n",
      "Logistic regression kappa:  0.2593143640504042\n"
     ]
    }
   ],
   "source": [
    "print(\"POS tags 4-gram features, sentences\")\n",
    "print(\"------\")\n",
    "print(\"SVM accuracy: \", np.mean(results_svm))\n",
    "print(\"SVM precision: \", precision_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM recall: \", recall_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM F1: \", f1_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM kappa: \", cohen_kappa_score(y_test, predicted_svm))\n",
    "print(\"------\")\n",
    "print(\"Logistic regression accuracy: \", np.mean(results_lr))\n",
    "print(\"Logistic regression precision: \", precision_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression recall: \", recall_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression F1: \", f1_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression kappa: \", cohen_kappa_score(y_test, predicted_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30875576, 0.48847926, 0.20276498],\n",
       "       [0.14030612, 0.70918367, 0.1505102 ],\n",
       "       [0.25925926, 0.42592593, 0.31481481]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = confusion_matrix(y_test, predicted_svm,['pos', 'neg','neu'])\n",
    "np.transpose( np.transpose(C) / C.astype(np.float).sum(axis=1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4516129 , 0.33179724, 0.21658986],\n",
       "       [0.19897959, 0.67346939, 0.12755102],\n",
       "       [0.31481481, 0.34259259, 0.34259259]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = confusion_matrix(y_test, predicted_lr,['pos', 'neg','neu'])\n",
    "np.transpose( np.transpose(C) / C.astype(np.float).sum(axis=1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-grams + TF-IDF, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 2191 instances, test on 731 instances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/indra/anaconda/envs/icutestenv/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 2191 instances, test on 731 instances\n",
      "train on 2191 instances, test on 731 instances\n",
      "train on 2193 instances, test on 729 instances\n"
     ]
    }
   ],
   "source": [
    "results_svm = []\n",
    "results_lr = []\n",
    "for i in k:\n",
    "    X_train,y_train,X_test,y_test = get_LP_ngram_train_test_features(language=\"cat\",k=i,n=4,POS=True,tf_idf=True)\n",
    "    accuracies = []\n",
    "    for s in random_states:\n",
    "        clf_svm = SGDClassifier(loss='hinge', alpha=1e-3, penalty='l2', random_state=s, max_iter=5,class_weight=\"balanced\")\n",
    "        clf_svm.fit(X_train, y_train) \n",
    "        predicted_svm = clf_svm.predict(X_test)\n",
    "        accuracies.append(np.mean(predicted_svm == y_test))\n",
    "    results_svm.append(np.mean(accuracies))\n",
    "\n",
    "    clf_lr = LogisticRegression(max_iter=5,class_weight=\"balanced\").fit(X_train, y_train)\n",
    "    predicted_lr = clf_lr.predict(X_test)\n",
    "    results_lr.append(np.mean(predicted_lr == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tags 4-gram + tf-idf features, sentences\n",
      "------\n",
      "SVM accuracy:  0.534770891355818\n",
      "SVM precision:  0.5458337534275337\n",
      "SVM recall:  0.5746164574616457\n",
      "SVM F1:  0.532788494388241\n",
      "SVM kappa:  0.20323754768332813\n",
      "------\n",
      "Logistic regression accuracy:  0.5507197637911054\n",
      "Logistic regression precision:  0.5367832898908882\n",
      "Logistic regression recall:  0.5509065550906556\n",
      "Logistic regression F1:  0.5416243956883869\n",
      "Logistic regression kappa:  0.21394159565014592\n"
     ]
    }
   ],
   "source": [
    "print(\"POS tags 4-gram + tf-idf features, sentences\")\n",
    "print(\"------\")\n",
    "print(\"SVM accuracy: \", np.mean(results_svm))\n",
    "print(\"SVM precision: \", precision_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM recall: \", recall_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM F1: \", f1_score(y_test, predicted_svm, average='weighted'))\n",
    "print(\"SVM kappa: \", cohen_kappa_score(y_test, predicted_svm))\n",
    "print(\"------\")\n",
    "print(\"Logistic regression accuracy: \", np.mean(results_lr))\n",
    "print(\"Logistic regression precision: \", precision_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression recall: \", recall_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression F1: \", f1_score(y_test, predicted_lr, average='weighted'))\n",
    "print(\"Logistic regression kappa: \", cohen_kappa_score(y_test, predicted_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22119816, 0.6359447 , 0.14285714],\n",
       "       [0.06887755, 0.85714286, 0.07397959],\n",
       "       [0.17592593, 0.56481481, 0.25925926]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = confusion_matrix(y_test, predicted_svm,['pos', 'neg','neu'])\n",
    "np.transpose( np.transpose(C) / C.astype(np.float).sum(axis=1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.359447  , 0.46543779, 0.17511521],\n",
       "       [0.17602041, 0.73214286, 0.09183673],\n",
       "       [0.28703704, 0.43518519, 0.27777778]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = confusion_matrix(y_test, predicted_lr,['pos', 'neg','neu'])\n",
    "np.transpose( np.transpose(C) / C.astype(np.float).sum(axis=1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
